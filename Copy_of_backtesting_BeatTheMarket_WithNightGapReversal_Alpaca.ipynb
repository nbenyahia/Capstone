{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyHzdEurX5Zt"
      },
      "source": [
        "## Beat the Market: An Effective Intraday Momentum Strategy for the S&P500 ETF (SPY) with Night Gap Reversal Enhancement\n",
        "\n",
        "This code allows you to replicate and extend the results presented in the paper \"Beat the Market: An Effective Intraday Momentum Strategy for the S&P500 ETF (SPY)\" by incorporating a night gap reversal strategy. The addition of this strategy aims to explore additional profit opportunities by leveraging overnight price movements.\n",
        "\n",
        "Published by ConcretumGroup founder, **Carlo Zarattini**. Co-authored by **Mohamed Gabriel**.\n",
        "\n",
        "For detailed explanations, please refer to the full blog post.\n",
        "\n",
        "You can reach the authors of this code by email at [info@concretumgroup.com](mailto:info@concretumgroup.com).\n",
        "\n",
        "More information can be found at [www.concretumgroup.com](http://www.concretumgroup.com).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oikDyjF0X5Zw"
      },
      "source": [
        "0. Needed Functions & Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YY4wAg1nX5Zw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from   datetime import datetime, timedelta,time\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "import statsmodels.api as sm\n",
        "import pytz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSmdWXbBX5Zx"
      },
      "outputs": [],
      "source": [
        "API_KEY_ID     = \"API_KEY_ID\"\n",
        "API_SECRET_KEY = \"API_SECRET_KEY\"\n",
        "\n",
        "def fetch_alpaca_data(symbol, timeframe, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Fetch stock bars data from Alpaca Markets based on the given timeframe.\n",
        "    \"\"\"\n",
        "    url = 'https://data.alpaca.markets/v2/stocks/bars'\n",
        "    headers = {\n",
        "        'APCA-API-KEY-ID': API_KEY_ID,\n",
        "        'APCA-API-SECRET-KEY': API_SECRET_KEY\n",
        "    }\n",
        "\n",
        "    params = {\n",
        "        'symbols': symbol,\n",
        "        'timeframe': timeframe,\n",
        "        'start': datetime.strptime(start_date, \"%Y-%m-%d\").isoformat() + 'Z',\n",
        "        'end': datetime.strptime(end_date, \"%Y-%m-%d\").isoformat() + 'Z',\n",
        "        'limit': 10000,\n",
        "        'adjustment': 'raw',\n",
        "        'feed': 'sip'\n",
        "    }\n",
        "\n",
        "    data_list = []\n",
        "    eastern = pytz.timezone('America/New_York')\n",
        "    utc = pytz.utc\n",
        "\n",
        "    market_open  = time(9, 30)  # Market opens at 9:30 AM\n",
        "    market_close = time(15, 59)  # Market closes just before 4:00 PM\n",
        "\n",
        "    print(\"Starting data fetch...\")\n",
        "    while True:\n",
        "        print(f\"Fetching data for symbols: {symbol} from {start_date} to {end_date}\")\n",
        "        response = requests.get(url, headers=headers, params=params)\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Error fetching data with status code {response.status_code}: {response.text}\")\n",
        "            break\n",
        "\n",
        "        data = response.json()\n",
        "\n",
        "        bars = data.get('bars')\n",
        "\n",
        "        for symbol, entries in bars.items():\n",
        "            print(f\"Processing {len(entries)} entries for symbol: {symbol}\")\n",
        "            for entry in entries:\n",
        "                try:\n",
        "                    utc_time = datetime.fromisoformat(entry['t'].rstrip('Z')).replace(tzinfo=utc)\n",
        "                    eastern_time = utc_time.astimezone(eastern)\n",
        "\n",
        "                    # Apply market hours filter for '1Min' timeframe\n",
        "                    if timeframe == '1Min' and not (market_open <= eastern_time.time() <= market_close):\n",
        "                        continue  # Skip entries outside market hours\n",
        "\n",
        "                    data_entry = {\n",
        "                        'volume': entry['v'],\n",
        "                        'open': entry['o'],\n",
        "                        'high': entry['h'],\n",
        "                        'low': entry['l'],\n",
        "                        'close': entry['c'],\n",
        "                        'caldt': eastern_time\n",
        "                    }\n",
        "                    data_list.append(data_entry)\n",
        "                    print(f\"Appended data for {symbol} at {eastern_time}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing entry: {entry}, {e}\")\n",
        "                    continue\n",
        "\n",
        "        if 'next_page_token' in data and data['next_page_token']:\n",
        "            params['page_token'] = data['next_page_token']\n",
        "            print(\"Fetching next page...\")\n",
        "        else:\n",
        "            print(\"No more pages to fetch.\")\n",
        "            break\n",
        "\n",
        "    df = pd.DataFrame(data_list)\n",
        "    print(\"Data fetching complete.\")\n",
        "    return df\n",
        "\n",
        "def fetch_alpaca_dividends(symbol, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Fetch dividend announcements from Alpaca for a specified symbol between two dates.\n",
        "    This function splits the request into manageable 90-day segments to comply with API constraints.\n",
        "    \"\"\"\n",
        "\n",
        "    url_base = \"https://paper-api.alpaca.markets/v2/corporate_actions/announcements\"\n",
        "    headers = {\n",
        "        \"accept\": \"application/json\",\n",
        "        \"APCA-API-KEY-ID\": API_KEY_ID,\n",
        "        \"APCA-API-SECRET-KEY\": API_SECRET_KEY\n",
        "    }\n",
        "\n",
        "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
        "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
        "\n",
        "    dividends_list = []\n",
        "    current_start = start_date\n",
        "\n",
        "    while current_start < end_date:\n",
        "\n",
        "\n",
        "        current_end = min(current_start + timedelta(days=89), end_date)\n",
        "\n",
        "        url = f\"{url_base}?ca_types=Dividend&since={current_start}&until={current_end}&symbol={symbol}\"\n",
        "\n",
        "        response = requests.get(url, headers=headers)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            for entry in data:\n",
        "                dividends_list.append({\n",
        "                    'caldt': datetime.strptime(entry['ex_date'], '%Y-%m-%d'),\n",
        "                    'dividend': float(entry['cash'])\n",
        "                })\n",
        "        else:\n",
        "            print(f\"Failed to fetch data for period {current_start} to {current_end}: {response.text}\")\n",
        "\n",
        "        current_start = current_end + timedelta(days=1)\n",
        "\n",
        "    return pd.DataFrame(dividends_list)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRaJQv8PX5Zy"
      },
      "source": [
        "1. Download Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9pArS9dX5Zy"
      },
      "outputs": [],
      "source": [
        "symbol = 'SPY'\n",
        "start_date     = '2018-01-01'\n",
        "end_date       = '2024-06-04'\n",
        "spy_intra_data = fetch_alpaca_data(symbol, '1Min', start_date, end_date)\n",
        "spy_daily_data = fetch_alpaca_data(symbol, '1D', start_date, end_date)\n",
        "dividends      = fetch_alpaca_dividends(symbol,start_date,end_date)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bRkcAcoX5Zy"
      },
      "source": [
        "2. ADD KEY VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V90yBl8JX5Zz"
      },
      "outputs": [],
      "source": [
        "# Load the intraday data into a DataFrame and set the datetime column as the index.\n",
        "df = pd.DataFrame(spy_intra_data)\n",
        "df['day'] = pd.to_datetime(df['caldt']).dt.date  # Extract the date part from the datetime for daily analysis.\n",
        "df.set_index('caldt', inplace=True)  # Setting the datetime as the index for easier time series manipulation.\n",
        "\n",
        "# Group the DataFrame by the 'day' column to facilitate operations that need daily aggregation.\n",
        "daily_groups = df.groupby('day')\n",
        "\n",
        "# Extract unique days from the dataset to iterate through each day for processing.\n",
        "all_days = df['day'].unique()\n",
        "\n",
        "# Initialize new columns to store calculated metrics, starting with NaN for absence of initial values.\n",
        "df['move_open'] = np.nan  # To record the absolute daily change from the open price\n",
        "df['vwap'] = np.nan       # To calculate the Volume Weighted Average Price.\n",
        "df['spy_dvol'] = np.nan   # To record SPY's daily volatility.\n",
        "\n",
        "# Create a series to hold computed daily returns for SPY, initialized with NaN.\n",
        "spy_ret = pd.Series(index=all_days, dtype=float)\n",
        "\n",
        "# Iterate through each day to calculate metrics.\n",
        "for d in range(1, len(all_days)):\n",
        "    current_day = all_days[d]\n",
        "    prev_day = all_days[d - 1]\n",
        "\n",
        "    # Access the data for the current and previous days using their groups.\n",
        "    current_day_data = daily_groups.get_group(current_day)\n",
        "    prev_day_data = daily_groups.get_group(prev_day)\n",
        "\n",
        "    # Calculate the average of high, low, and close prices.\n",
        "    hlc = (current_day_data['high'] + current_day_data['low'] + current_day_data['close']) / 3\n",
        "\n",
        "    # Compute volume-weighted metrics for VWAP calculation.\n",
        "    vol_x_hlc = current_day_data['volume'] * hlc\n",
        "    cum_vol_x_hlc = vol_x_hlc.cumsum()  # Cumulative sum for VWAP calculation.\n",
        "    cum_volume = current_day_data['volume'].cumsum()\n",
        "\n",
        "    # Assign the calculated VWAP to the corresponding index in the DataFrame.\n",
        "    df.loc[current_day_data.index, 'vwap'] = cum_vol_x_hlc / cum_volume\n",
        "\n",
        "    # Calculate the absolute percentage change from the day's opening price.\n",
        "    open_price = current_day_data['open'].iloc[0]\n",
        "    df.loc[current_day_data.index, 'move_open'] = (current_day_data['close'] / open_price - 1).abs()\n",
        "\n",
        "    # Compute the daily return for SPY using the closing prices from the current and previous day.\n",
        "    spy_ret.loc[current_day] = current_day_data['close'].iloc[-1] / prev_day_data['close'].iloc[-1] - 1\n",
        "\n",
        "    # Calculate the 15-day rolling volatility, starting calculation after accumulating 15 days of data.\n",
        "    if d > 14:\n",
        "        df.loc[current_day_data.index, 'spy_dvol'] = spy_ret.iloc[d - 15:d].std(skipna=False)\n",
        "\n",
        "# Calculate the minutes from market open and determine the minute of the day for each timestamp.\n",
        "df['min_from_open'] = ((df.index - df.index.normalize()) / pd.Timedelta(minutes=1)) - (9 * 60 + 30) + 1\n",
        "df['minute_of_day'] = df['min_from_open'].round().astype(int)\n",
        "\n",
        "# Group data by 'minute_of_day' for minute-level calculations.\n",
        "minute_groups = df.groupby('minute_of_day')\n",
        "\n",
        "# Calculate rolling mean and delayed sigma for each minute of the trading day.\n",
        "df['move_open_rolling_mean'] = minute_groups['move_open'].transform(lambda x: x.rolling(window=14, min_periods=13).mean())\n",
        "df['sigma_open'] = minute_groups['move_open_rolling_mean'].transform(lambda x: x.shift(1))\n",
        "\n",
        "# Convert dividend dates to datetime and merge dividend data based on trading days.\n",
        "dividends['day'] = pd.to_datetime(dividends['caldt']).dt.date\n",
        "df = df.merge(dividends[['day', 'dividend']], on='day', how='left')\n",
        "df['dividend'] = df['dividend'].fillna(0)  # Fill missing dividend data with 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gt-j_PKBX5Zz"
      },
      "source": [
        "3. BACKTEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSC7H-YwX5Zz"
      },
      "outputs": [],
      "source": [
        "# Constants and settings\n",
        "AUM_0 = 100000.0\n",
        "commission = 0.0035\n",
        "min_comm_per_order = 0.35\n",
        "band_mult = 1\n",
        "trade_freq = 30\n",
        "sizing_type = \"vol_target\"\n",
        "target_vol = 0.02\n",
        "max_leverage = 4\n",
        "overnight_threshold = 0.02\n",
        "\n",
        "# Group data by day for faster access\n",
        "daily_groups = df.groupby('day')\n",
        "\n",
        "# Initialize strategy DataFrame using unique days\n",
        "strat = pd.DataFrame(index=all_days)\n",
        "strat['ret'] = np.nan\n",
        "strat['AUM'] = AUM_0\n",
        "strat['ret_spy'] = np.nan\n",
        "\n",
        "# Calculate daily returns for SPY using the closing prices\n",
        "df_daily = pd.DataFrame(spy_daily_data)\n",
        "df_daily['caldt'] = pd.to_datetime(df_daily['caldt']).dt.date\n",
        "df_daily.set_index('caldt', inplace=True)  # Set the datetime column as the DataFrame index for easy time series manipulation.\n",
        "\n",
        "df_daily['ret'] = df_daily['close'].diff() / df_daily['close'].shift()\n",
        "\n",
        "\n",
        "# Loop through all days, starting from the second day\n",
        "for d in range(1, len(all_days)):\n",
        "    current_day = all_days[d]\n",
        "    prev_day = all_days[d-1]\n",
        "\n",
        "    if prev_day in daily_groups.groups and current_day in daily_groups.groups:\n",
        "        prev_day_data = daily_groups.get_group(prev_day)\n",
        "        current_day_data = daily_groups.get_group(current_day)\n",
        "\n",
        "        if 'sigma_open' in current_day_data.columns and current_day_data['sigma_open'].isna().all():\n",
        "            continue\n",
        "\n",
        "        prev_close_adjusted = prev_day_data['close'].iloc[-1] - df.loc[current_day_data.index, 'dividend'].iloc[-1]\n",
        "\n",
        "        open_price = current_day_data['open'].iloc[0]\n",
        "        current_close_prices = current_day_data['close']\n",
        "        spx_vol = current_day_data['spy_dvol'].iloc[0]\n",
        "        vwap = current_day_data['vwap']\n",
        "\n",
        "        sigma_open = current_day_data['sigma_open']\n",
        "        UB = max(open_price, prev_close_adjusted) * (1 + band_mult * sigma_open)\n",
        "        LB = min(open_price, prev_close_adjusted) * (1 - band_mult * sigma_open)\n",
        "\n",
        "        # Determine trading signals\n",
        "        signals = np.zeros_like(current_close_prices)\n",
        "        signals[(current_close_prices > UB) & (current_close_prices > vwap)] = 1\n",
        "        signals[(current_close_prices < LB) & (current_close_prices < vwap)] = -1\n",
        "\n",
        "\n",
        "        # Position sizing\n",
        "        previous_aum = strat.loc[prev_day, 'AUM']\n",
        "\n",
        "        if sizing_type == \"vol_target\":\n",
        "            if math.isnan(spx_vol):\n",
        "                shares = round(previous_aum / open_price * max_leverage)\n",
        "            else:\n",
        "                shares = round(previous_aum / open_price * min(target_vol / spx_vol, max_leverage))\n",
        "\n",
        "        elif sizing_type == \"full_notional\":\n",
        "            shares = round(previous_aum / open_price)\n",
        "\n",
        "        # Apply trading signals at trade frequencies\n",
        "        trade_indices = np.where(current_day_data[\"min_from_open\"] % trade_freq == 0)[0]\n",
        "        exposure = np.full(len(current_day_data), np.nan)  # Start with NaNs\n",
        "        exposure[trade_indices] = signals[trade_indices]  # Apply signals at trade times\n",
        "\n",
        "        # Custom forward-fill that stops at zeros\n",
        "        last_valid = np.nan  # Initialize last valid value as NaN\n",
        "        filled_values = []   # List to hold the forward-filled values\n",
        "        for value in exposure:\n",
        "            if not np.isnan(value):  # If current value is not NaN, update last valid value\n",
        "                last_valid = value\n",
        "            if last_valid == 0:  # Reset if last valid value is zero\n",
        "                last_valid = np.nan\n",
        "            filled_values.append(last_valid)\n",
        "\n",
        "        exposure = pd.Series(filled_values, index=current_day_data.index).shift(1).fillna(0).values  # Apply shift and fill NaNs\n",
        "\n",
        "        # Calculate trades count based on changes in exposure\n",
        "        trades_count = np.sum(np.abs(np.diff(np.append(exposure, 0))))\n",
        "\n",
        "        # Calculate the percentage change from the previous day's adjusted closing price to today's opening price\n",
        "        overnight_move = (open_price / prev_close_adjusted - 1)\n",
        "        # Determine the trade signal based on whether the overnight move is greater than the threshold; trade against the move\n",
        "        open_trade_signal = -np.sign(overnight_move) * (abs(overnight_move) > overnight_threshold)\n",
        "\n",
        "        trade_time_row = current_day_data[current_day_data['min_from_open'] == trade_freq]\n",
        "        exit_price_minute_version_trade = trade_time_row['close'].iloc[0]\n",
        "\n",
        "        # Calculate PnL of Mean-Reversion Portfolio (MRP)\n",
        "        pnl_mean_reversion_trade = open_trade_signal * shares * (exit_price_minute_version_trade - open_price)\n",
        "        comm_mean_reversion_trade = 2 * max(min_comm_per_order, commission * shares) * abs(open_trade_signal)\n",
        "        net_pnl_mean_reversion = pnl_mean_reversion_trade - comm_mean_reversion_trade\n",
        "\n",
        "        # Calculate PnL of Intraday Momentum Portfolio (IMP)\n",
        "        change_1m = current_close_prices.diff()\n",
        "        gross_pnl = np.sum(exposure * change_1m) * shares\n",
        "        commission_paid = trades_count * max(min_comm_per_order, commission * shares)\n",
        "        net_pnl_mom = gross_pnl - commission_paid\n",
        "\n",
        "        # Calculate the aggregate daily PnL summing the PnL of MRP and PnL of IMP\n",
        "        net_pnl = net_pnl_mom + net_pnl_mean_reversion\n",
        "\n",
        "        # Update the daily return and new AUM\n",
        "        strat.loc[current_day, 'AUM'] = previous_aum + net_pnl\n",
        "        strat.loc[current_day, 'ret'] = net_pnl / previous_aum\n",
        "\n",
        "        # Save the passive Buy&Hold daily return for SPY\n",
        "        strat.loc[current_day, 'ret_spy'] = df_daily.loc[df_daily.index == current_day, 'ret'].values[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ly08D2NDX5Z0"
      },
      "source": [
        "4. Study Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH_a5LvJX5Z0"
      },
      "outputs": [],
      "source": [
        "# Calculate cumulative products for AUM calculations\n",
        "strat['AUM_SPX'] = AUM_0 * (1 + strat['ret_spy']).cumprod(skipna=True)\n",
        "\n",
        "# Create a figure and a set of subplots\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# Plotting the AUM of the strategy and the passive S&P 500 exposure\n",
        "ax.plot(strat.index[::20], strat['AUM'].iloc[::20], label='Momentum + MeanReversion', linewidth=2, color='k')\n",
        "ax.plot(strat.index[::20], strat['AUM_SPX'].iloc[::20], label=f'{symbol} Buy & Hold', linewidth=1, color='r')\n",
        "\n",
        "# Formatting the plot\n",
        "ax.grid(True, linestyle=':')\n",
        "ax.xaxis.set_major_locator(mdates.YearLocator())\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "plt.xticks(rotation=90)\n",
        "ax.set_ylabel('AUM ($)')\n",
        "\n",
        "plt.legend(loc='upper left')\n",
        "plt.title('Intraday Momentum Strategy+MeanReversion Strategy', fontsize=12, fontweight='bold')\n",
        "plt.suptitle(f'Commission = ${commission}/share', fontsize=9, verticalalignment='top')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n",
        "# Calculate additional stats and display them\n",
        "stats = {\n",
        "    'Total Return (%)': round((np.prod(1 + strat['ret'].dropna()) - 1) * 100, 0),\n",
        "    'Annualized Return (%)': round((np.prod(1 + strat['ret']) ** (252 / len(strat['ret'])) - 1) * 100, 1),\n",
        "    'Annualized Volatility (%)': round(strat['ret'].dropna().std() * np.sqrt(252) * 100, 1),\n",
        "    'Sharpe Ratio': round(strat['ret'].dropna().mean() / strat['ret'].dropna().std() * np.sqrt(252), 2),\n",
        "    'Hit Ratio (%)': round((strat['ret'] > 0).sum() / (strat['ret'].abs() > 0).sum() * 100, 0),\n",
        "    'Maximum Drawdown (%)': round(strat['AUM'].div(strat['AUM'].cummax()).sub(1).min() * -100, 0)\n",
        "}\n",
        "\n",
        "\n",
        "Y = strat['ret'].dropna()\n",
        "X = sm.add_constant(strat['ret_spy'].dropna())\n",
        "model = sm.OLS(Y, X).fit()\n",
        "stats['Alpha (%)'] = round(model.params.const * 100 * 252, 2)\n",
        "stats['Beta'] = round(model.params['ret_spy'], 2)\n",
        "\n",
        "print(stats)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}